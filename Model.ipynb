{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nfrom pickle import load\nimport pickle\n\n\nimport copy\nimport tensorflow as tf\n\npath = '../input/cnn_dataset.pkl'\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['__output__.json', 'custom.css', 'cnn_dataset.pkl', '__results__.html']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "stories = load(open(path, 'rb'))\nprint('Loaded Stories %d' % len(stories))\n\n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Loaded Stories 92579\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1356c65d8b1a67e26e3563805b41f451d475a67c"
      },
      "cell_type": "code",
      "source": "print(stories[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de5209107cd39d94e294e360b27df665e3bc838f"
      },
      "cell_type": "code",
      "source": "#\ndef load_data(path):\n    stories = load(open(path, 'rb'))\n    text_sentences = ''\n    summary_sentences = ''\n    stories = stories[:2400]\n    for i in range(len(stories)):\n        text_sentences += ' '.join(stories[i]['story'])\n        summary_sentences += ' '.join(stories[i]['highlights'])\n        if i != len(stories)-1:\n            text_sentences += '\\n'\n            summary_sentences += '\\n'\n    return text_sentences, summary_sentences",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32d5f399325c1714c32df1591c9931a2a4551da5"
      },
      "cell_type": "code",
      "source": "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n\ndef create_lookup_tables(text):\n    # make a list of unique words\n    vocab = set(text.split())\n\n    # (1)\n    # starts with the special tokens\n    vocab_to_int = copy.copy(CODES)\n\n    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n    # since vocab_to_int already contains special tokens\n    for v_i, v in enumerate(vocab, len(CODES)):\n        vocab_to_int[v] = v_i\n\n    # (2)\n    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n\n    return vocab_to_int, int_to_vocab",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cad7c273bf25499638d310c4410bee2ae8e0dfed"
      },
      "cell_type": "code",
      "source": "text = text_sentences\ntable = create_lookup_tables(text)\nfor k, v in table[0].items():\n    print(k, v)\n    if v == 20:\n        break\nprint(len(table[0]))",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'text_sentences' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-371f9b54faec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_lookup_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_sentences' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7cee6f8911cae850fb9588e8d9088c1858a50d0"
      },
      "cell_type": "code",
      "source": "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n    \"\"\"\n        1st, 2nd args: raw string text to be converted\n        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n    \n        return: A tuple of lists (source_id_text, target_id_text) converted\n    \"\"\"\n    # empty list of converted sentences\n    source_text_id = []\n    target_text_id = []\n    \n    # make a list of sentences (extraction)\n    source_sentences = source_text.split(\"\\n\")\n    target_sentences = target_text.split(\"\\n\")\n    \n    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n    \n    # iterating through each sentences (# of sentences in source&target is the same)\n    for i in range(len(source_sentences)):\n        # extract sentences one by one\n        source_sentence = source_sentences[i]\n        target_sentence = target_sentences[i]\n        # make a list of tokens/words (extraction) from the chosen sentence\n        source_tokens = source_sentence.split(\" \")\n        target_tokens = target_sentence.split(\" \")\n        \n        # empty list of converted words to index in the chosen sentence\n        source_token_id = []\n        target_token_id = []\n        \n        for index, token in enumerate(source_tokens):\n            if (token != \"\"):\n                source_token_id.append(source_vocab_to_int[token])\n        \n        for index, token in enumerate(target_tokens):\n            if (token != \"\"):\n                target_token_id.append(target_vocab_to_int[token])\n                \n        # put <EOS> token at the end of the chosen target sentence\n        # this token suggests when to stop creating a sequence\n        target_token_id.append(target_vocab_to_int['<EOS>'])\n            \n        # add each converted sentences in the final list\n        source_text_id.append(source_token_id)\n        target_text_id.append(target_token_id)\n    \n    return source_text_id, target_text_id",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0c211836f3337a05300a2d4b4b01414ad282c68"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8df20c56328c49f4c247f2e7df0fb23e3ab8e659"
      },
      "cell_type": "code",
      "source": "def preprocess_and_save_data(path):\n    # Preprocess\n    \n    # load original data (text, summary)\n    source_text, target_text = load_data(path)\n\n\n    # create lookup tables for text and summary\n    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n\n    # create list of sentences whose words are represented in index\n    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n\n    # Save data for later use\n    pickle.dump((\n        (source_text, target_text),\n        (source_vocab_to_int, target_vocab_to_int),\n        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b11bc5bb2c5acf4f0b5d018c4c67bc11c411a6fa"
      },
      "cell_type": "code",
      "source": "preprocess_and_save_data(path)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b33f85eba6047bab62510ce3e11d78feeaa86934"
      },
      "cell_type": "code",
      "source": "import pickle\n\ndef load_preprocess():\n    with open('preprocess.p', mode='rb') as in_file:\n        return pickle.load(in_file)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f7c95954acdfc12e1e692f24e81ca7e910e281b"
      },
      "cell_type": "code",
      "source": "import numpy as np\n\n(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e63d8d2a5062d7c74acdecfc67ac53e29a68e14"
      },
      "cell_type": "code",
      "source": "# testing preprocessing\nprint(type(source_int_text))\nprint(source_int_text[0])\nprint(target_int_text[0])\nprint(type(source_vocab_to_int))\nfor k, v in source_vocab_to_int.items():\n    print(k, v)\n    if v == 10:\n        break",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'list'>\n[36862, 27297, 42494, 10875, 12286, 14945, 20183, 20111, 17677, 43602, 7812, 4852, 42241, 9103, 9103, 30310, 1111, 43802, 37310, 40294, 19784, 10536, 43802, 33615, 20448, 34066, 23616, 15625, 4852, 33061, 40294, 458, 20183, 20111, 1111, 10875, 12286, 42553, 10875, 12286, 36254, 20448, 11815, 35314, 20448, 34642, 7812, 20448, 20994, 35314, 31059, 19763, 15549, 20448, 12168, 20448, 14036, 8760, 39875, 43802, 1202, 24067, 35878, 7812, 478, 40872, 35167, 11372, 40294, 19940, 29541, 16625, 36254, 18060, 38101, 8050, 35155, 44793, 7812, 41050, 25315, 35314, 7407, 22987, 30076, 1111, 5460, 1111, 29089, 9014, 10185, 7247, 36909, 4852, 14359, 2194, 10357, 6726, 11057, 42860, 41318, 43802, 37292, 18734, 35603, 3653, 908, 16030, 26721, 8050, 35155, 42135, 2918, 41050, 24235, 42867, 20448, 27674, 8050, 15058, 43300, 44976, 27674, 37013, 16537, 14871, 44510, 4800, 36254, 697, 36254, 4031, 21371, 20111, 1974, 41050, 24886, 7812, 10074, 42553, 4852, 22853, 7247, 4852, 16625, 42553, 4852, 16945, 11057, 12158, 20448, 13612, 45849, 24822, 22306, 9858, 29806, 35415, 5154, 25562, 40872, 36943, 25315, 95, 40294, 20448, 44861, 3760, 29252, 43571, 45085, 35314, 40273, 19986, 37500, 40872, 30825, 11496, 44976, 30804, 18680, 40872, 21484, 36254, 17007, 2079, 24067, 2303, 40294, 40872, 35878, 2918, 7771, 35132, 15662, 10997, 35314, 33969, 43802, 3246, 42553, 18734, 19807, 15764, 15362, 35314, 1286, 1974, 41050, 31645, 14871, 20465, 16950, 28163, 35547, 20448, 9008, 22513, 12286, 21858, 35132, 16844, 13864, 42553, 4852, 29530, 27352, 9988, 43802, 1202, 30862, 16844, 24886, 7812, 30644, 33832, 40872, 5114, 6353, 36254, 45926, 20448, 8760, 39875, 43802, 41449, 6703, 40872, 36503, 33402, 40294, 20448, 14321, 22987, 44976, 3893, 39712, 7812, 23266, 2083, 14945, 36015, 27719, 20448, 9860, 43802, 37292, 2616, 39903, 26721, 10536, 8050, 29705, 9464, 30611, 30862, 7812, 21795, 37013, 43817, 24181, 20465, 38826, 41050, 16798, 19048, 26210, 41480, 40872, 16271, 5587, 1111, 20448, 37239, 40787, 42553, 10875, 12286, 36254, 20448, 11815, 35314, 20448, 34642, 41449, 32277, 29925, 40294, 12702, 1414, 35314, 20448, 40637, 36254, 43802, 1974, 27140, 20448, 43720, 42553, 20448, 39903, 45480, 21858, 37821, 45216, 42419, 25714, 35369, 35314, 672, 16271, 20840, 41449, 19992, 31323, 12286, 44272, 20448, 8004, 1202, 6092, 4852, 748, 95, 20912, 10892, 37239, 40735, 14871, 17624, 30950, 37053, 36254, 40872, 15335, 10602, 26210, 35730, 40550, 16030, 27307, 43802, 1974, 27151, 13148, 42553, 17429, 23775, 18734, 35603, 45849, 14871, 31478, 23775, 30076, 37864, 18734, 17113, 908, 16030, 27307, 43802, 23786, 40872, 25123, 41173, 15919, 4852, 45462, 10388, 42553, 6035, 21805, 37447, 5365, 43802, 41449, 36259, 26210, 17341, 23561, 31104, 14812, 43797, 37013, 22838, 2344, 18734, 5974, 8050, 21271, 42135, 12299, 42569, 7812, 41050, 20903, 10997, 35314, 44399, 17822, 43802, 37292, 42494, 11994, 7812, 4852, 27890, 34221, 42494, 12866, 43693, 44641, 27543, 37646, 30862, 41050, 26169, 14407, 7903, 11057, 18114]\n[12699, 821, 3848, 10070, 10002, 2412, 8445, 1249, 10686, 3127, 9606, 2776, 10267, 6107, 10686, 1343, 1191, 1465, 9104, 565, 7240, 690, 13328, 6181, 5904, 1102, 10555, 12797, 821, 12009, 649, 6068, 2589, 9224, 7498, 4927, 1]\n<class 'dict'>\n<PAD> 0\n<EOS> 1\n<UNK> 2\n<GO> 3\nlegislative 4\nacknowledging 5\nphenom 6\nflyover 7\nharassing 8\nskidded 9\nalison 10\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e65d2937cca1a6bf5956ddf169b59ddae56667d"
      },
      "cell_type": "code",
      "source": "def enc_dec_model_inputs():\n    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n    \n    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n    max_target_len = tf.reduce_max(target_sequence_length)    \n    \n    return inputs, targets, target_sequence_length, max_target_len",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8b6d50c6fa52b0ae0716660e66c5c845bd605352"
      },
      "cell_type": "code",
      "source": "def hyperparam_inputs():\n    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n    \n    return lr_rate, keep_prob",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a9303f0924019d9e6167ee4a82c045c86f7d1d1"
      },
      "cell_type": "code",
      "source": "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n    \"\"\"\n    Preprocess target data for encoding\n    :return: Preprocessed target data\n    \"\"\"\n    # get '<GO>' id\n    go_id = target_vocab_to_int['<GO>']\n    \n    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n    \n    return after_concat",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb03452c746bd243385d089826aec6097c529471"
      },
      "cell_type": "code",
      "source": "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n                   source_vocab_size, \n                   encoding_embedding_size):\n    \"\"\"\n    :return: tuple (RNN output, RNN state)\n    \"\"\"\n    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n                                             vocab_size=source_vocab_size, \n                                             embed_dim=encoding_embedding_size)\n    \n    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n    \n    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n                                       embed, \n                                       dtype=tf.float32)\n    return outputs, state",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9607aab73510631b62f9201fff26cf53c9b2c7a"
      },
      "cell_type": "code",
      "source": "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n                         target_sequence_length, max_summary_length, \n                         output_layer, keep_prob):\n    \"\"\"\n    Create a training process in decoding layer \n    :return: BasicDecoderOutput containing training logits and sample_id\n    \"\"\"\n    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n                                             output_keep_prob=keep_prob)\n    \n    # for only input layer\n    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n                                               target_sequence_length)\n    \n    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n                                              helper, \n                                              encoder_state, \n                                              output_layer)\n\n    # unrolling the decoder layer\n    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n                                                      impute_finished=True, \n                                                      maximum_iterations=max_summary_length)\n    return outputs",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85e27424ff965783f93da791b6a5b831b75c45c7"
      },
      "cell_type": "code",
      "source": "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n                         end_of_sequence_id, max_target_sequence_length,\n                         vocab_size, output_layer, batch_size, keep_prob):\n    \"\"\"\n    Create a inference process in decoding layer \n    :return: BasicDecoderOutput containing inference logits and sample_id\n    \"\"\"\n    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n                                             output_keep_prob=keep_prob)\n    \n    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n                                                      tf.fill([batch_size], start_of_sequence_id), \n                                                      end_of_sequence_id)\n    \n    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n                                              helper, \n                                              encoder_state, \n                                              output_layer)\n    \n    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n                                                      impute_finished=True, \n                                                      maximum_iterations=max_target_sequence_length)\n    return outputs",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a24a34cdd73cafa603d3e8eb55b2b05865be993"
      },
      "cell_type": "code",
      "source": "def decoding_layer(dec_input, encoder_state,\n                   target_sequence_length, max_target_sequence_length,\n                   rnn_size,\n                   num_layers, target_vocab_to_int, target_vocab_size,\n                   batch_size, keep_prob, decoding_embedding_size):\n    \"\"\"\n    Create decoding layer\n    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n    \"\"\"\n    target_vocab_size = len(target_vocab_to_int)\n    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n    \n    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n    \n    with tf.variable_scope(\"decode\"):\n        output_layer = tf.layers.Dense(target_vocab_size)\n        train_output = decoding_layer_train(encoder_state, \n                                            cells, \n                                            dec_embed_input, \n                                            target_sequence_length, \n                                            max_target_sequence_length, \n                                            output_layer, \n                                            keep_prob)\n    with tf.variable_scope(\"decode\", reuse=True):\n        infer_output = decoding_layer_infer(encoder_state, \n                                            cells, \n                                            dec_embeddings, \n                                            target_vocab_to_int['<GO>'], \n                                            target_vocab_to_int['<EOS>'], \n                                            max_target_sequence_length, \n                                            target_vocab_size, \n                                            output_layer,\n                                            batch_size,\n                                            keep_prob)\n\n    return (train_output, infer_output)",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8879fb2247a8b625571009a1b6926cc2936ec836"
      },
      "cell_type": "code",
      "source": "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n                  target_sequence_length,\n                  max_target_sentence_length,\n                  source_vocab_size, target_vocab_size,\n                  enc_embedding_size, dec_embedding_size,\n                  rnn_size, num_layers, target_vocab_to_int):\n    \"\"\"\n    Build the Sequence-to-Sequence model\n    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n    \"\"\"\n    enc_outputs, enc_states = encoding_layer(input_data, \n                                             rnn_size, \n                                             num_layers, \n                                             keep_prob, \n                                             source_vocab_size, \n                                             enc_embedding_size)\n    \n    dec_input = process_decoder_input(target_data, \n                                      target_vocab_to_int, \n                                      batch_size)\n    \n    train_output, infer_output = decoding_layer(dec_input,\n                                               enc_states, \n                                               target_sequence_length, \n                                               max_target_sentence_length,\n                                               rnn_size,\n                                              num_layers,\n                                              target_vocab_to_int,\n                                              target_vocab_size,\n                                              batch_size,\n                                              keep_prob,\n                                              dec_embedding_size)\n    \n    return train_output, infer_output",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d05b663e11164cad1c637a01e8a68a8ff6822568"
      },
      "cell_type": "code",
      "source": "display_step = 150\n\nepochs = 5\nbatch_size = 8\n\nrnn_size = 64\nnum_layers = 3\n#num_layers = 1\n\nencoding_embedding_size = 200\ndecoding_embedding_size = 200\n\nlearning_rate = 0.001\nkeep_probability = 0.5",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee470d4a05991d44f7d1caf488a77591de8defeb"
      },
      "cell_type": "code",
      "source": "save_path = 'checkpoints/dev'\n(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\nmax_target_sentence_length = max([len(sentence) for sentence in target_int_text])\n\ntrain_graph = tf.Graph()\nwith train_graph.as_default():\n    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n    lr, keep_prob = hyperparam_inputs()\n    \n    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n                                                   targets,\n                                                   keep_prob,\n                                                   batch_size,\n                                                   target_sequence_length,\n                                                   max_target_sequence_length,\n                                                   len(source_vocab_to_int),\n                                                   len(target_vocab_to_int),\n                                                   encoding_embedding_size,\n                                                   decoding_embedding_size,\n                                                   rnn_size,\n                                                   num_layers,\n                                                   target_vocab_to_int)\n    \n    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n    \n    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n    # - Returns a mask tensor representing the first N positions of each cell.\n    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n\n    with tf.name_scope(\"optimization\"):\n        # Loss function - weighted softmax cross entropy\n        cost = tf.contrib.seq2seq.sequence_loss(\n            training_logits,\n            targets,\n            masks)\n\n        # Optimizer\n        optimizer = tf.train.AdamOptimizer(lr)\n\n        # Gradient Clipping\n        gradients = optimizer.compute_gradients(cost)\n        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n        train_op = optimizer.apply_gradients(capped_gradients)",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0730299c59fea157eaa0003c500f8ce5a93038f8"
      },
      "cell_type": "code",
      "source": "def pad_sentence_batch(sentence_batch, pad_int):\n    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n    max_sentence = max([len(sentence) for sentence in sentence_batch])\n    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n\n\ndef get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n    for batch_i in range(0, len(sources)//batch_size):\n        start_i = batch_i * batch_size\n\n        # Slice the right amount for the batch\n        sources_batch = sources[start_i:start_i + batch_size]\n        targets_batch = targets[start_i:start_i + batch_size]\n\n        # Pad\n        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n\n        # Need the lengths for the _lengths parameters\n        pad_targets_lengths = []\n        for target in pad_targets_batch:\n            pad_targets_lengths.append(len(target))\n\n        pad_source_lengths = []\n        for source in pad_sources_batch:\n            pad_source_lengths.append(len(source))\n\n        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7407b94d4dad94cb1fa759af3d7f295694a43ea5"
      },
      "cell_type": "code",
      "source": "def get_accuracy(target, logits):\n    \"\"\"\n    Calculate accuracy\n    \"\"\"\n    max_seq = max(target.shape[1], logits.shape[1])\n    if max_seq - target.shape[1]:\n        target = np.pad(\n            target,\n            [(0,0),(0,max_seq - target.shape[1])],\n            'constant')\n    if max_seq - logits.shape[1]:\n        logits = np.pad(\n            logits,\n            [(0,0),(0,max_seq - logits.shape[1])],\n            'constant')\n\n    return np.mean(np.equal(target, logits))\n\n# Split data to training and validation sets\ntrain_source = source_int_text[batch_size:]\ntrain_target = target_int_text[batch_size:]\nvalid_source = source_int_text[:batch_size]\nvalid_target = target_int_text[:batch_size]\n(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n                                                                                                             valid_target,\n                                                                                                             batch_size,\n                                                                                                             source_vocab_to_int['<PAD>'],\n                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \nwith tf.Session(graph=train_graph) as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for epoch_i in range(epochs):\n        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n                get_batches(train_source, train_target, batch_size,\n                            source_vocab_to_int['<PAD>'],\n                            target_vocab_to_int['<PAD>'])):\n\n            _, loss = sess.run(\n                [train_op, cost],\n                {input_data: source_batch,\n                 targets: target_batch,\n                 lr: learning_rate,\n                 target_sequence_length: targets_lengths,\n                 keep_prob: keep_probability})\n            \n            if batch_i % display_step == 0 and batch_i > 0:\n                batch_train_logits = sess.run(\n                    inference_logits,\n                    {input_data: source_batch,\n                     target_sequence_length: targets_lengths,\n                     keep_prob: 1.0})\n\n                batch_valid_logits = sess.run(\n                    inference_logits,\n                    {input_data: valid_sources_batch,\n                     target_sequence_length: valid_targets_lengths,\n                     keep_prob: 1.0})\n\n                train_acc = get_accuracy(target_batch, batch_train_logits)\n                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n\n                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n\n    # Save Model\n    saver = tf.train.Saver()\n    saver.save(sess, save_path)\n    print('Model Trained and Saved')",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch   0 Batch  150/300 - Train Accuracy: 0.2296, Validation Accuracy: 0.1862, Loss: 6.5974\nEpoch   1 Batch  150/300 - Train Accuracy: 0.2449, Validation Accuracy: 0.1811, Loss: 5.9969\nEpoch   2 Batch  150/300 - Train Accuracy: 0.2551, Validation Accuracy: 0.1888, Loss: 5.8992\nEpoch   3 Batch  150/300 - Train Accuracy: 0.2526, Validation Accuracy: 0.1939, Loss: 5.8450\nEpoch   4 Batch  150/300 - Train Accuracy: 0.2551, Validation Accuracy: 0.1862, Loss: 5.8263\nModel Trained and Saved\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4aef18603e20d7127c6c4a31ddb8970bdd829053"
      },
      "cell_type": "code",
      "source": "def save_params(params):\n    with open('params.p', 'wb') as out_file:\n        pickle.dump(params, out_file)\n\n\ndef load_params():\n    with open('params.p', mode='rb') as in_file:\n        return pickle.load(in_file)",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "365ad1bf76e2ee58eaeb8459a1ae4a17ae077209"
      },
      "cell_type": "code",
      "source": "# Save parameters for checkpoint\nsave_params(save_path)",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7791f7d6d68366f2ac3a918d24bf765b6450c75"
      },
      "cell_type": "code",
      "source": "#import problem_unittests as tests\n\n_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\nload_path = load_params()",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed6b6b724baf3da9491fcc964930247d991915d8"
      },
      "cell_type": "code",
      "source": "def sentence_to_seq(sentence, vocab_to_int):\n    results = []\n    for word in sentence.split(\" \"):\n        if word in vocab_to_int:\n            results.append(vocab_to_int[word])\n        else:\n            results.append(vocab_to_int['<UNK>'])\n            \n    return results",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "180af13c1914dc0336dbd394c03de110cddb3139"
      },
      "cell_type": "code",
      "source": "translate_sentence = ' '.join(stories[24001]['story'])\n\ntranslate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n                                \nloaded_graph = tf.Graph()\nwith tf.Session(graph=loaded_graph) as sess:\n    # Load saved model\n    loader = tf.train.import_meta_graph(load_path + '.meta')\n    loader.restore(sess, load_path)\n\n    input_data = loaded_graph.get_tensor_by_name('input:0')\n    logits = loaded_graph.get_tensor_by_name('predictions:0')\n    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n\n    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n                                         keep_prob: 1.0})[0]\nprint('Input')\n#print('  Word Ids:      {}'.format([i for i in translate_sentence]))\nprint('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n\nprint('\\nPrediction')\n#print('  Word Ids:      {}'.format([i for i in translate_logits]))\nprint('  Summary: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
